Key project components:

Data cleaning and exploration:
Calculates distinct orderItemOrderId values in order_items.
Determines row counts for orders and order_items.
Sales analysis:
Identifies top-selling products and categories based on canceled orders, aggregating total sales amounts.
Determines peak sales months and days of the week, providing insights into sales trends.
Data warehousing:
Consolidates data from all retail_db tables into a comprehensive retail_all table in a PostgreSQL database, facilitating future analysis.
The resulting table is optimized for querying and reporting, providing a valuable resource for business intelligence.
This project showcases the power of Spark for efficient data processing and analysis. By combining data cleaning, transformation, and warehousing techniques, it offers a comprehensive solution for gaining valuable insights from retail transaction data.

Key Technologies: Spark, Python, PySpark, PostgreSQL, SQL